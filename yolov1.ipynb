{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms,datasets\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create import create_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list_path=r'C:\\Users\\ASUS RYZEN 7\\Documents\\PROYECTOS\\Computer vision\\animas\\agri_data\\images.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos los archivos .txt se han movido a C:\\Users\\ASUS RYZEN 7\\Documents\\PROYECTOS\\Computer vision\\animas\\agri_data\\images\\labels\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Ruta de la carpeta que contiene los archivos .txt\n",
    "source_dir = r'C:\\Users\\ASUS RYZEN 7\\Documents\\PROYECTOS\\Computer vision\\animas\\agri_data\\images'\n",
    "\n",
    "# Crear la carpeta 'labels' si no existe\n",
    "labels_dir = os.path.join(source_dir, 'labels')\n",
    "os.makedirs(labels_dir, exist_ok=True)\n",
    "\n",
    "# Iterar sobre todos los archivos en el directorio\n",
    "for filename in os.listdir(source_dir):\n",
    "    if filename.endswith('.txt'):  # Si es un archivo .txt\n",
    "        source_path = os.path.join(source_dir, filename)\n",
    "        destination_path = os.path.join(labels_dir, filename)\n",
    "        \n",
    "        # Mover el archivo .txt a la carpeta 'labels'\n",
    "        shutil.copy(source_path, destination_path)\n",
    "\n",
    "print(f'Todos los archivos .txt se han movido a {labels_dir}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_images_txt(image_folder, output_txt):\n",
    "    # Abrir el archivo para escribir\n",
    "    with open(output_txt, \"w\") as file:\n",
    "        # Recorrer todos los archivos en el folder de imágenes\n",
    "        for filename in os.listdir(image_folder):\n",
    "            # Si el archivo es una imagen (.jpg o .jpeg o .png)\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n",
    "                # Escribir la ruta completa de la imagen en el archivo\n",
    "                img_path = os.path.join(image_folder, filename)\n",
    "                file.write(f\"{img_path}\\n\")\n",
    "\n",
    "# Especifica la carpeta de imágenes y el archivo de salida\n",
    "image_folder = r'C:\\Users\\ASUS RYZEN 7\\Documents\\PROYECTOS\\Computer vision\\animas\\agri_data\\images'\n",
    "output_txt = r'C:\\Users\\ASUS RYZEN 7\\Documents\\PROYECTOS\\Computer vision\\animas\\agri_data\\images.txt'\n",
    "\n",
    "create_images_txt(image_folder, output_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from utils import encode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloDataset(Dataset):\n",
    "    def __init__(self, img_list_path, S, B, num_classes, transforms=None, img_box_transforms=None, eval_mode=False):\n",
    "        with open(img_list_path, \"r\") as img_list_file:\n",
    "            self.img_filenames = img_list_file.readlines()\n",
    "            \n",
    "        self.img_filenames = list(map(lambda x:x.strip(), self.img_filenames))\n",
    "        self.label_files = []\n",
    "        for path in self.img_filenames:\n",
    "            image_dir = os.path.dirname(path)\n",
    "            label_dir = \"labels\".join(image_dir.rsplit(\"images\", 1))\n",
    "            assert label_dir != image_dir, \\\n",
    "                f\"Image path must contain a folder named 'images'! \\n'{image_dir}'\"\n",
    "            label_file = os.path.join(r'C:\\Users\\ASUS RYZEN 7\\Documents\\PROYECTOS\\Computer vision\\animas\\agri_data\\images', os.path.basename(path))\n",
    "            label_file = os.path.splitext(label_file)[0] + '.txt'\n",
    "            self.label_files.append(label_file)\n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.img_box_transforms = img_box_transforms\n",
    "        \n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.num_classes = num_classes\n",
    "        self.eval_mode = eval_mode\n",
    "\n",
    "    def eval(self, eval_mode=True):\n",
    "        self.eval_mode = eval_mode\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read image\n",
    "        img_filename = self.img_filenames[idx]\n",
    "        img = Image.open(img_filename, mode='r')\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        # read each image's corresponding label (.txt)\n",
    "        labels = []\n",
    "        with open(self.label_files[idx], 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                if line == '\\n':\n",
    "                    continue\n",
    "                line = line.strip().split(' ')\n",
    "                # convert xywh str to float, class str to int\n",
    "                c, x, y, w, h = int(line[0]), float(line[1]), float(line[2]), float(line[3]), float(line[4])\n",
    "                if self.eval_mode: \n",
    "                    labels.append((x, y, w, h, 1.0, c))\n",
    "                else:\n",
    "                    labels.append((x, y, w, h, c))\n",
    "                \n",
    "        if self.img_box_transforms is not None:\n",
    "            for t in self.img_box_transforms:\n",
    "                img, labels = t(img, labels)\n",
    "        \n",
    "        if self.eval_mode: \n",
    "            return img, torch.Tensor(labels)\n",
    "\n",
    "        encoded_labels = encode(labels, self.S, self.B, self.num_classes)  # convert label list to encoded label\n",
    "        encoded_labels = torch.Tensor(encoded_labels)\n",
    "        return img, encoded_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = YoloDataset(img_list_path, 7, 2, 2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data loader\n",
    "train_loader, val_loader = create_dataloaders(img_list_path, 0.8, 0.2, 16, 448, 7, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000,  ..., 0.4314, 0.4275, 0.0000],\n",
       "          [0.4667, 0.4824, 0.4863,  ..., 0.4353, 0.4353, 0.0000],\n",
       "          [0.4510, 0.4706, 0.4824,  ..., 0.4392, 0.4392, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.5490, 0.5373,  ..., 0.3216, 0.3333, 0.3412],\n",
       "          [0.0000, 0.5490, 0.5490,  ..., 0.3176, 0.3255, 0.3333],\n",
       "          [0.0000, 0.5529, 0.5608,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.6118, 0.6118, 0.0000],\n",
       "          [0.4549, 0.4706, 0.4745,  ..., 0.6118, 0.6118, 0.0000],\n",
       "          [0.4510, 0.4667, 0.4784,  ..., 0.6118, 0.6157, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.4980, 0.4980,  ..., 0.2627, 0.2784, 0.2863],\n",
       "          [0.0000, 0.5020, 0.5098,  ..., 0.2588, 0.2706, 0.2784],\n",
       "          [0.0000, 0.5059, 0.5137,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.5451, 0.5373, 0.0000],\n",
       "          [0.5176, 0.5373, 0.5569,  ..., 0.5529, 0.5412, 0.0000],\n",
       "          [0.4980, 0.5294, 0.5490,  ..., 0.5412, 0.5451, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.4667, 0.4667,  ..., 0.2549, 0.2706, 0.2784],\n",
       "          [0.0000, 0.4706, 0.4784,  ..., 0.2510, 0.2588, 0.2706],\n",
       "          [0.0000, 0.4745, 0.4863,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.5490, 0.5647,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5686, 0.5686,  ..., 0.6235, 0.5922, 0.5725],\n",
       "          [0.0000, 0.5961, 0.5961,  ..., 0.6353, 0.6039, 0.5882],\n",
       "          ...,\n",
       "          [0.5882, 0.5059, 0.4784,  ..., 0.0588, 0.0510, 0.0000],\n",
       "          [0.6039, 0.5059, 0.4784,  ..., 0.0196, 0.0118, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0353, 0.0275, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.8196, 0.8275,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.8314, 0.8314,  ..., 0.5843, 0.5529, 0.5294],\n",
       "          [0.0000, 0.8431, 0.8431,  ..., 0.5961, 0.5647, 0.5451],\n",
       "          ...,\n",
       "          [0.5647, 0.4784, 0.4510,  ..., 0.0392, 0.0235, 0.0000],\n",
       "          [0.5765, 0.4784, 0.4510,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0078, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.8039, 0.8039,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.8000, 0.8000,  ..., 0.5333, 0.5020, 0.4902],\n",
       "          [0.0000, 0.7961, 0.7961,  ..., 0.5490, 0.5137, 0.5059],\n",
       "          ...,\n",
       "          [0.4745, 0.3882, 0.3608,  ..., 0.0314, 0.0235, 0.0000],\n",
       "          [0.4980, 0.3882, 0.3608,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Abre tu imagen usando PIL\n",
    "image = Image.open('image3.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(Conv, self).__init__()\n",
    "        # Definimos la convolución\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        # Usamos ReLU como función de activación\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Aplicamos la convolución y luego la activación ReLU\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "# Definición de la clase Conv para una capa convolucional personalizada con LRN\n",
    "class ConvLRN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(ConvLRN, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.lrn = nn.LocalResponseNorm(size=5, alpha=1e-4, beta=0.75, k=2.0)  # LRN\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lrn(x)  # Aplicar LRN después de la activación\n",
    "        return x\n",
    "    \n",
    "class ConvBN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0):\n",
    "        super(ConvBN, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)  # Batch Normalization\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "        \n",
    "class MaxPooling(nn.Module):\n",
    "    def __init__(self,kernel_size=3,stride=2,padding=0):\n",
    "        super(MaxPooling,self).__init__()\n",
    "        self.maxpool=nn.MaxPool2d(kernel_size=kernel_size,stride=stride,padding=padding)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.maxpool(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOV1(nn.Module):\n",
    "    def __init__(self,S,B,num_classes):\n",
    "        super(YOLOV1,self).__init__()\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.layer1=Conv(kernel_size=7,in_channels=3,out_channels=64,stride=2)\n",
    "        self.maxpool1=MaxPooling(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.layer2=Conv(kernel_size=3,in_channels=64,out_channels=192,stride=1)\n",
    "        self.maxpool2=MaxPooling(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.layer3=Conv(kernel_size=1,in_channels=192,out_channels=128,stride=1)\n",
    "        self.layer4=Conv(kernel_size=3,in_channels=128,out_channels=256,stride=1)\n",
    "        self.layer5=Conv(kernel_size=1,in_channels=256,out_channels=256,stride=1)\n",
    "        self.layer6=Conv(kernel_size=3,in_channels=256,out_channels=512,stride=1)\n",
    "        self.maxpool3=MaxPooling(kernel_size=2,stride=2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.layer7=Conv(kernel_size=1,in_channels=512,out_channels=256,stride=1)\n",
    "        self.layer8=Conv(kernel_size=3,in_channels=256,out_channels=512,stride=1)\n",
    "        \n",
    "        self.layer9=Conv(kernel_size=1,in_channels=512,out_channels=256,stride=1)\n",
    "        self.layer10=Conv(kernel_size=3,in_channels=256,out_channels=512,stride=1)\n",
    "        \n",
    "        self.layer11=Conv(kernel_size=1,in_channels=512,out_channels=256,stride=1)\n",
    "        self.layer12=Conv(kernel_size=3,in_channels=256,out_channels=512,stride=1)\n",
    "        \n",
    "        self.layer13=Conv(kernel_size=1,in_channels=512,out_channels=256,stride=1)\n",
    "        self.layer14=Conv(kernel_size=3,in_channels=256,out_channels=512,stride=1)\n",
    "        \n",
    "        self.layer15=Conv(kernel_size=1,in_channels=512,out_channels=512,stride=1)\n",
    "        self.layer16=Conv(kernel_size=3,in_channels=512,out_channels=1024,stride=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.maxpool4=MaxPooling(kernel_size=2,stride=2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.layer17=Conv(kernel_size=1,in_channels=1024,out_channels=512,stride=1)\n",
    "        self.layer18=Conv(kernel_size=3,in_channels=512,out_channels=1024,stride=1)\n",
    "        \n",
    "        self.layer19=Conv(kernel_size=1,in_channels=1024,out_channels=512,stride=1)\n",
    "        self.layer20=Conv(kernel_size=3,in_channels=512,out_channels=1024,stride=1)\n",
    "        \n",
    "        self.layer21=Conv(kernel_size=3,in_channels=1024,out_channels=1024,stride=1)\n",
    "        \n",
    "        self.layer22=Conv(kernel_size=3,in_channels=1024,out_channels=1024,stride=2)\n",
    "        \n",
    "        self.layer23=Conv(kernel_size=3,in_channels=1024,out_channels=1024,stride=1)\n",
    "        self.layer24=Conv(kernel_size=3,in_channels=1024,out_channels=1024,stride=1)\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * 9* 9, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, self.S * self.S * (self.B * 5 + self.num_classes))  # Output de 7x7x30\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x=self.layer1(x)\n",
    "        x=self.maxpool1(x)\n",
    "        \n",
    "        x=self.layer2(x)\n",
    "        x=self.maxpool2(x)\n",
    "        \n",
    "        \n",
    "        x=self.layer3(x)\n",
    "        x=self.layer4(x)\n",
    "        x=self.layer5(x)\n",
    "        x=self.layer6(x)\n",
    "        x=self.maxpool3(x)\n",
    "        \n",
    "        x=self.layer7(x)\n",
    "        x=self.layer8(x)\n",
    "        \n",
    "        x=self.layer9(x)\n",
    "        x=self.layer10(x)\n",
    "        \n",
    "        x=self.layer11(x)\n",
    "        x=self.layer12(x)\n",
    "        \n",
    "        x=self.layer13(x)\n",
    "        x=self.layer14(x)\n",
    "        \n",
    "        x=self.layer15(x)\n",
    "        x=self.layer16(x)\n",
    "        x=self.maxpool4(x)\n",
    "        \n",
    "        x=self.layer17(x)\n",
    "        x=self.layer18(x)\n",
    "        \n",
    "        x=self.layer19(x)\n",
    "        x=self.layer20(x)\n",
    "        \n",
    "        x=self.layer21(x)\n",
    "        x=self.layer22(x)\n",
    "        \n",
    "        x=self.layer23(x)\n",
    "        x=self.layer24(x)\n",
    "        \n",
    "        # Aplana el tensor para que tenga el tamaño adecuado\n",
    "\n",
    "        \n",
    "        x=self.fc1(x)\n",
    "        \n",
    "          # Reestructuramos la salida a 7x7x30\n",
    "        x = x.reshape(-1, self.S, self.S, self.B * 5 + self.num_classes)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "    \n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = YOLOV1(7,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torchvision import utils\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from utils import YoloLoss, load_yaml, metrics\n",
    "from datasets import create_dataloaders\n",
    "\n",
    "parser = argparse.ArgumentParser(description='YOLO')\n",
    "parser.add_argument(\"--type\", \"-t\", default=\"ms\", help=\"model type\", type=str)\n",
    "parser.add_argument(\"--cfg\", \"-c\", default=\"config/model.yaml\", help=\"model config file\", type=str)\n",
    "parser.add_argument(\"--weights\", \"-w\", default=\"\", help=\"model weights\", type=str)\n",
    "parser.add_argument(\"--dataset\", \"-d\", default=\"config/dataset.yaml\", help=\"dataset config file\", type=str)\n",
    "parser.add_argument(\"--epochs\", \"-e\", default=135, help=\"number of epochs\", type=int)\n",
    "parser.add_argument(\"--lr\", \"-lr\", default=0.0005, help=\"learning rate\", type=float)\n",
    "parser.add_argument(\"--batch_size\", \"-bs\", default=64, help=\"batch size\", type=int)\n",
    "parser.add_argument(\"--output\", \"-o\", default=\"output\", help=\"output dir\", type=str)\n",
    "parser.add_argument(\"--save\", \"-sv\", default=5, help=\"save model checkpoint every #nth epoch\", type=int)\n",
    "parser.add_argument('--tboard', action='store_true', default=True, help='use tensorboard')\n",
    "parser.add_argument(\"--cuda\", \"-cu\", action='store_true', default=False, help='use cuda')\n",
    "parser.add_argument(\"--sched_lr\", \"-sc\", action='store_true', default=False, help='use scheduler')\n",
    "\n",
    "# parse arguments\n",
    "args = parser.parse_args()\n",
    "\n",
    "torch.manual_seed(32)\n",
    "\n",
    "def train(model, train_loader, optimizer, epoch, scheduler, device, S, B, train_loss_lst, writer):\n",
    "    model.train()  # Set the module in training mode\n",
    "    train_loss = 0\n",
    "    pbar = tqdm(train_loader, leave=True)\n",
    "    for batch_idx, (inputs, labels) in enumerate(pbar):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # back prop\n",
    "        criterion = YoloLoss(S, B)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if args.sched_lr:\n",
    "            scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # log batch0 images\n",
    "        if batch_idx == 0 and epoch == 0:\n",
    "            inputs = inputs.cpu()  # convert to cpu\n",
    "            img_grid = utils.make_grid(inputs)\n",
    "            writer.add_image('image batch0', img_grid, 0)\n",
    "\n",
    "        # print loss and accuracy\n",
    "        pbar.set_description(f\"[Epoch {epoch+1}] loss = {train_loss/(batch_idx+1):.03f}\")\n",
    "\n",
    "    # record training loss\n",
    "    train_loss /= len(pbar)\n",
    "    train_loss_lst.append(train_loss)\n",
    "\n",
    "    tqdm.write(f\"Epoch {epoch} training summary -- loss = {train_loss:.03f}\")\n",
    "    return train_loss_lst\n",
    "\n",
    "\n",
    "def validate(model, val_loader, device, S, B, valid_loss_list):\n",
    "    model.eval()  # Sets the module in evaluation mode\n",
    "    val_loss = 0\n",
    "    pbar = tqdm(val_loader, leave=True)\n",
    "    with torch.no_grad(): # without gradient calculation\n",
    "        for data, target in pbar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            criterion = YoloLoss(S, B)\n",
    "            val_loss += criterion(output, target).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    tqdm.write(f\"Epoch {epoch} validation summary -- loss = {val_loss:.03f}\")\n",
    "   \n",
    "    valid_loss_list.append(val_loss)\n",
    "    return valid_loss_list\n",
    "\n",
    "\n",
    "cfg = load_yaml(args.cfg)\n",
    "print('cfg:', cfg)\n",
    "dataset = load_yaml(args.dataset)\n",
    "print('dataset:', dataset)\n",
    "\n",
    "img_list_path = dataset['images']\n",
    "S, B, num_classes, input_size = cfg['S'], cfg['B'], cfg['num_classes'], cfg['input_size']\n",
    "\n",
    "# create output file folder\n",
    "start = time.strftime('%Y-%m-%d-%H-%M-%S', time.localtime(time.time()))\n",
    "output_dir = os.path.join(args.output, 'train', start)\n",
    "os.makedirs(output_dir)\n",
    "\n",
    "if args.cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# build model\n",
    "model = YOLOV1(S, B, num_classes).to(device)\n",
    "\n",
    "# get data loader\n",
    "train_loader, val_loader = create_dataloaders(img_list_path, 0.8, 0.2, args.batch_size, input_size, S, B, num_classes)\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "scheduler = None\n",
    "if args.sched_lr:\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=0.01, epochs=args.epochs, steps_per_epoch=len(train_loader), anneal_strategy='cos')\n",
    "\n",
    "train_loss_lst, valid_loss_list = [], []\n",
    "\n",
    "print('using tensorboard...')\n",
    "print('to run tesnsorboard server, run: \\'tensorboard --logdir output/train/ --bind_all\\'')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "writer = SummaryWriter(output_dir)\n",
    "\n",
    "# train epoch\n",
    "for epoch in range(args.epochs):\n",
    "    train_loss_lst = train(model, train_loader, optimizer, epoch, scheduler, device, S, B, train_loss_lst, writer)\n",
    "    valid_loss_list = validate(model, val_loader, device, S, B, valid_loss_list)\n",
    "    writer.add_scalar('Loss/train', np.average(train_loss_lst), epoch)\n",
    "    writer.add_scalar('Loss/validate', np.average(valid_loss_list), epoch)\n",
    "\n",
    "    # save model weight every save_freq epoch\n",
    "    if epoch % args.save_freq == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(output_dir, 'epoch' + str(epoch) + '.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, 'last.pt'))\n",
    "\n",
    "# save loss plot\n",
    "fig = plt.figure()\n",
    "plt.plot(range(args.epochs), train_loss_lst, 'g', label='train loss')\n",
    "plt.plot(range(args.epochs), valid_loss_list, 'r', label='val loss')\n",
    "plt.grid(True)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig(os.path.join(output_dir, 'loss_plot.jpg'))\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 7, 12])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define la transformación para redimensionar la imagen a 227x227\n",
    "resize_transform = transforms.Compose([\n",
    "    transforms.Resize((227, 227)),  # Cambiamos el tamaño a 227x227\n",
    "    transforms.ToTensor()           # Convertimos la imagen a un tensor\n",
    "])\n",
    "\n",
    "# Aplica la transformación a la imagen\n",
    "input_image = resize_transform(image)\n",
    "\n",
    "# Añade una dimensión para el batch (porque AlexNet espera el tamaño (batch_size, 3, 227, 227))\n",
    "input_image = input_image.unsqueeze(0)  # Esto convierte la imagen a (1, 3, 227, 227)\n",
    "\n",
    "# Pasa la imagen a través del modelo AlexNet\n",
    "output = model(input_image)\n",
    "\n",
    "# Imprime la forma de la salida\n",
    "print(output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class Loss(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_size=7, num_bboxes=2, num_classes=2, lambda_coord=5.0, lambda_noobj=0.5):\n",
    "        \"\"\" Constructor.\n",
    "        Args:\n",
    "            feature_size: (int) size of input feature map.\n",
    "            num_bboxes: (int) number of bboxes per each cell.\n",
    "            num_classes: (int) number of the object classes.\n",
    "            lambda_coord: (float) weight for bbox location/size losses.\n",
    "            lambda_noobj: (float) weight for no-objectness loss.\n",
    "        \"\"\"\n",
    "        super(Loss, self).__init__()\n",
    "\n",
    "        self.S = feature_size\n",
    "        self.B = num_bboxes\n",
    "        self.C = num_classes\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "\n",
    "\n",
    "    def compute_iou(self, bbox1, bbox2):\n",
    "        \"\"\" Compute the IoU (Intersection over Union) of two set of bboxes, each bbox format: [x1, y1, x2, y2].\n",
    "        Args:\n",
    "            bbox1: (Tensor) bounding bboxes, sized [N, 4].\n",
    "            bbox2: (Tensor) bounding bboxes, sized [M, 4].\n",
    "        Returns:\n",
    "            (Tensor) IoU, sized [N, M].\n",
    "        \"\"\"\n",
    "        N = bbox1.size(0)\n",
    "        M = bbox2.size(0)\n",
    "\n",
    "        # Compute left-top coordinate of the intersections\n",
    "        lt = torch.max(\n",
    "            bbox1[:, :2].unsqueeze(1).expand(N, M, 2), # [N, 2] -> [N, 1, 2] -> [N, M, 2]\n",
    "            bbox2[:, :2].unsqueeze(0).expand(N, M, 2)  # [M, 2] -> [1, M, 2] -> [N, M, 2]\n",
    "        )\n",
    "        # Conpute right-bottom coordinate of the intersections\n",
    "        rb = torch.min(\n",
    "            bbox1[:, 2:].unsqueeze(1).expand(N, M, 2), # [N, 2] -> [N, 1, 2] -> [N, M, 2]\n",
    "            bbox2[:, 2:].unsqueeze(0).expand(N, M, 2)  # [M, 2] -> [1, M, 2] -> [N, M, 2]\n",
    "        )\n",
    "        # Compute area of the intersections from the coordinates\n",
    "        wh = rb - lt   # width and height of the intersection, [N, M, 2]\n",
    "        wh[wh < 0] = 0 # clip at 0\n",
    "        inter = wh[:, :, 0] * wh[:, :, 1] # [N, M]\n",
    "\n",
    "        # Compute area of the bboxes\n",
    "        area1 = (bbox1[:, 2] - bbox1[:, 0]) * (bbox1[:, 3] - bbox1[:, 1]) # [N, ]\n",
    "        area2 = (bbox2[:, 2] - bbox2[:, 0]) * (bbox2[:, 3] - bbox2[:, 1]) # [M, ]\n",
    "        area1 = area1.unsqueeze(1).expand_as(inter) # [N, ] -> [N, 1] -> [N, M]\n",
    "        area2 = area2.unsqueeze(0).expand_as(inter) # [M, ] -> [1, M] -> [N, M]\n",
    "\n",
    "        # Compute IoU from the areas\n",
    "        union = area1 + area2 - inter # [N, M, 2]\n",
    "        iou = inter / union           # [N, M, 2]\n",
    "\n",
    "        return iou\n",
    "\n",
    "    def forward(self, pred_tensor, target_tensor):\n",
    "        \"\"\" Compute loss for YOLO training.\n",
    "        Args:\n",
    "            pred_tensor: (Tensor) predictions, sized [n_batch, S, S, Bx5+C], 5=len([x, y, w, h, conf]).\n",
    "            target_tensor: (Tensor) targets, sized [n_batch, S, S, Bx5+C].\n",
    "        Returns:\n",
    "            (Tensor): loss, sized [1, ].\n",
    "        \"\"\"\n",
    "        # TODO: Romove redundant dimensions for some Tensors.\n",
    "\n",
    "        S, B, C = self.S, self.B, self.C\n",
    "        N = 5 * B + C    # 5=len([x, y, w, h, conf]\n",
    "\n",
    "        batch_size = pred_tensor.size(0)\n",
    "        coord_mask = target_tensor[:, :, :, 4] > 0  # mask for the cells which contain objects. [n_batch, S, S]\n",
    "        noobj_mask = target_tensor[:, :, :, 4] == 0 # mask for the cells which do not contain objects. [n_batch, S, S]\n",
    "        coord_mask = coord_mask.unsqueeze(-1).expand_as(target_tensor) # [n_batch, S, S] -> [n_batch, S, S, N]\n",
    "        noobj_mask = noobj_mask.unsqueeze(-1).expand_as(target_tensor) # [n_batch, S, S] -> [n_batch, S, S, N]\n",
    "\n",
    "        coord_pred = pred_tensor[coord_mask].view(-1, N)            # pred tensor on the cells which contain objects. [n_coord, N]\n",
    "                                                                    # n_coord: number of the cells which contain objects.\n",
    "        bbox_pred = coord_pred[:, :5*B].contiguous().view(-1, 5)    # [n_coord x B, 5=len([x, y, w, h, conf])]\n",
    "        class_pred = coord_pred[:, 5*B:]                            # [n_coord, C]\n",
    "\n",
    "        coord_target = target_tensor[coord_mask].view(-1, N)        # target tensor on the cells which contain objects. [n_coord, N]\n",
    "                                                                    # n_coord: number of the cells which contain objects.\n",
    "        bbox_target = coord_target[:, :5*B].contiguous().view(-1, 5)# [n_coord x B, 5=len([x, y, w, h, conf])]\n",
    "        class_target = coord_target[:, 5*B:]                        # [n_coord, C]\n",
    "\n",
    "        # Compute loss for the cells with no object bbox.\n",
    "        noobj_pred = pred_tensor[noobj_mask].view(-1, N)        # pred tensor on the cells which do not contain objects. [n_noobj, N]\n",
    "                                                                # n_noobj: number of the cells which do not contain objects.\n",
    "        noobj_target = target_tensor[noobj_mask].view(-1, N)    # target tensor on the cells which do not contain objects. [n_noobj, N]\n",
    "                                                                # n_noobj: number of the cells which do not contain objects.\n",
    "        noobj_conf_mask = torch.cuda.ByteTensor(noobj_pred.size()).fill_(0) # [n_noobj, N]\n",
    "        for b in range(B):\n",
    "            noobj_conf_mask[:, 4 + b*5] = 1 # noobj_conf_mask[:, 4] = 1; noobj_conf_mask[:, 9] = 1\n",
    "        noobj_pred_conf = noobj_pred[noobj_conf_mask]       # [n_noobj, 2=len([conf1, conf2])]\n",
    "        noobj_target_conf = noobj_target[noobj_conf_mask]   # [n_noobj, 2=len([conf1, conf2])]\n",
    "        loss_noobj = F.mse_loss(noobj_pred_conf, noobj_target_conf, reduction='sum')\n",
    "\n",
    "        # Compute loss for the cells with objects.\n",
    "        coord_response_mask = torch.cuda.ByteTensor(bbox_target.size()).fill_(0)    # [n_coord x B, 5]\n",
    "        coord_not_response_mask = torch.cuda.ByteTensor(bbox_target.size()).fill_(1)# [n_coord x B, 5]\n",
    "        bbox_target_iou = torch.zeros(bbox_target.size()).cuda()                    # [n_coord x B, 5], only the last 1=(conf,) is used\n",
    "\n",
    "        # Choose the predicted bbox having the highest IoU for each target bbox.\n",
    "        for i in range(0, bbox_target.size(0), B):\n",
    "            pred = bbox_pred[i:i+B] # predicted bboxes at i-th cell, [B, 5=len([x, y, w, h, conf])]\n",
    "            pred_xyxy = Variable(torch.FloatTensor(pred.size())) # [B, 5=len([x1, y1, x2, y2, conf])]\n",
    "            # Because (center_x,center_y)=pred[:, 2] and (w,h)=pred[:,2:4] are normalized for cell-size and image-size respectively,\n",
    "            # rescale (center_x,center_y) for the image-size to compute IoU correctly.\n",
    "            pred_xyxy[:,  :2] = pred[:, :2]/float(S) - 0.5 * pred[:, 2:4]\n",
    "            pred_xyxy[:, 2:4] = pred[:, :2]/float(S) + 0.5 * pred[:, 2:4]\n",
    "\n",
    "            target = bbox_target[i] # target bbox at i-th cell. Because target boxes contained by each cell are identical in current implementation, enough to extract the first one.\n",
    "            target = bbox_target[i].view(-1, 5) # target bbox at i-th cell, [1, 5=len([x, y, w, h, conf])]\n",
    "            target_xyxy = Variable(torch.FloatTensor(target.size())) # [1, 5=len([x1, y1, x2, y2, conf])]\n",
    "            # Because (center_x,center_y)=target[:, 2] and (w,h)=target[:,2:4] are normalized for cell-size and image-size respectively,\n",
    "            # rescale (center_x,center_y) for the image-size to compute IoU correctly.\n",
    "            target_xyxy[:,  :2] = target[:, :2]/float(S) - 0.5 * target[:, 2:4]\n",
    "            target_xyxy[:, 2:4] = target[:, :2]/float(S) + 0.5 * target[:, 2:4]\n",
    "\n",
    "            iou = self.compute_iou(pred_xyxy[:, :4], target_xyxy[:, :4]) # [B, 1]\n",
    "            max_iou, max_index = iou.max(0)\n",
    "            max_index = max_index.data.cuda()\n",
    "\n",
    "            coord_response_mask[i+max_index] = 1\n",
    "            coord_not_response_mask[i+max_index] = 0\n",
    "\n",
    "            # \"we want the confidence score to equal the intersection over union (IOU) between the predicted box and the ground truth\"\n",
    "            # from the original paper of YOLO.\n",
    "            bbox_target_iou[i+max_index, torch.LongTensor([4]).cuda()] = (max_iou).data.cuda()\n",
    "        bbox_target_iou = Variable(bbox_target_iou).cuda()\n",
    "\n",
    "        # BBox location/size and objectness loss for the response bboxes.\n",
    "        bbox_pred_response = bbox_pred[coord_response_mask].view(-1, 5)      # [n_response, 5]\n",
    "        bbox_target_response = bbox_target[coord_response_mask].view(-1, 5)  # [n_response, 5], only the first 4=(x, y, w, h) are used\n",
    "        target_iou = bbox_target_iou[coord_response_mask].view(-1, 5)        # [n_response, 5], only the last 1=(conf,) is used\n",
    "        loss_xy = F.mse_loss(bbox_pred_response[:, :2], bbox_target_response[:, :2], reduction='sum')\n",
    "        loss_wh = F.mse_loss(torch.sqrt(bbox_pred_response[:, 2:4]), torch.sqrt(bbox_target_response[:, 2:4]), reduction='sum')\n",
    "        loss_obj = F.mse_loss(bbox_pred_response[:, 4], target_iou[:, 4], reduction='sum')\n",
    "\n",
    "        # Class probability loss for the cells which contain objects.\n",
    "        loss_class = F.mse_loss(class_pred, class_target, reduction='sum')\n",
    "\n",
    "        # Total loss\n",
    "        loss = self.lambda_coord * (loss_xy + loss_wh) + loss_obj + self.lambda_noobj * loss_noobj + loss_class\n",
    "        loss = loss / float(batch_size)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, split='train', split_ratio=0.8, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.split = split\n",
    "        self.split_ratio = split_ratio\n",
    "        self.transform = transform\n",
    "        \n",
    "        # List all files in the directory\n",
    "        all_files = os.listdir(image_dir)\n",
    "        all_images = [file_name for file_name in all_files if file_name.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        # Split into train and validation\n",
    "        num_images = len(all_images)\n",
    "        split_index = int(num_images * split_ratio)\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.image_paths = all_images[:split_index]\n",
    "        else:\n",
    "            self.image_paths = all_images[split_index:]\n",
    "        \n",
    "        self.image_paths = [os.path.join(image_dir, file_name) for file_name in self.image_paths]\n",
    "        self.label_paths = [os.path.join(image_dir, file_name.rsplit('.', 1)[0] + '.txt') for file_name in self.image_paths]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        label_path = self.label_paths[index]\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = np.array(image).astype(np.float32) / 255.0\n",
    "        \n",
    "        # Load label\n",
    "        boxes = []\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                class_id = int(parts[0])\n",
    "                x_center, y_center, width, height = map(float, parts[1:])\n",
    "\n",
    "                # Convert normalized coordinates to absolute\n",
    "                boxes.append([class_id, x_center, y_center, width, height])\n",
    "        \n",
    "        if len(boxes) > 0:\n",
    "            boxes = np.array(boxes, dtype=np.float32)\n",
    "        else:\n",
    "            boxes = np.zeros((0, 5), dtype=np.float32)\n",
    "        \n",
    "        # Apply transformation if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, boxes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, targets = zip(*batch)\n",
    "    \n",
    "    # Convertir imágenes a tensor si son numpy.ndarray\n",
    "    if isinstance(imgs[0], np.ndarray):\n",
    "        imgs = [torch.tensor(img, dtype=torch.float32).permute(2, 0, 1) for img in imgs]\n",
    "    \n",
    "    # Convertir objetivos a tensor si son numpy.ndarray\n",
    "    if isinstance(targets[0], np.ndarray):\n",
    "        targets = [torch.tensor(target, dtype=torch.float32) for target in targets]\n",
    "    \n",
    "    imgs = torch.stack(imgs, 0)  # Apilar imágenes en un tensor\n",
    "    targets = torch.stack(targets, 0)  # Apilar objetivos en un tensor\n",
    "    \n",
    "    return imgs, targets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "Number of training images:  1040\n",
      "Number of validation images:  260\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "# Check if GPU devices are available.\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('CUDA available:', use_gpu)\n",
    "if use_gpu:\n",
    "    \n",
    "    print('CUDA current_device: {}'.format(torch.cuda.current_device()))\n",
    "    print('CUDA device_count: {}'.format(torch.cuda.device_count()))\n",
    "\n",
    "# Path to data dir.\n",
    "image_dir = 'C:/Users/ASUS RYZEN 7/Documents/PROYECTOS/Computer vision/animas/agri_data/data'\n",
    "\n",
    "# Path to checkpoint file containing pre-trained DarkNet weight.\n",
    "checkpoint_path = 'weights/darknet/model_best.pth.tar'\n",
    "\n",
    "# Frequency to print/log the results.\n",
    "print_freq = 5\n",
    "tb_log_freq = 5\n",
    "\n",
    "# Training hyper parameters.\n",
    "init_lr = 0.001\n",
    "base_lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 5.0e-4\n",
    "num_epochs = 4\n",
    "batch_size = 16\n",
    "\n",
    "# Learning rate scheduling.\n",
    "def update_lr(optimizer, epoch, burnin_base, burnin_exp=4.0):\n",
    "    if epoch == 0:\n",
    "        lr = init_lr + (base_lr - init_lr) * math.pow(burnin_base, burnin_exp)\n",
    "    elif epoch == 1:\n",
    "        lr = base_lr\n",
    "    elif epoch == 75:\n",
    "        lr = 0.001\n",
    "    elif epoch == 105:\n",
    "        lr = 0.0001\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "# Load YOLO model.\n",
    "yolo = model\n",
    "\n",
    "# Setup loss and optimizer.\n",
    "# Crear el modelo\n",
    "\n",
    "\n",
    "# Calcular el feature size pasando un tamaño de entrada (por ejemplo, 448x448)\n",
    "input_size = 448  # Asumiendo que esta es la resolución de entrada\n",
    "feature_size = yolo.get_feature_size(input_size)\n",
    "\n",
    "# Configurar el criterio de pérdida\n",
    "criterion = Loss(feature_size=feature_size)\n",
    "\n",
    "optimizer = torch.optim.SGD(yolo.parameters(), lr=init_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Definir la transformación para el dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((448, 448)),  # Redimensionar imagen\n",
    "    transforms.ToTensor(),  # Convertir imagen a tensor\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(image_dir, split='train', transform=transform)\n",
    "val_dataset = CustomDataset(image_dir, split='val', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print('Number of training images: ', len(train_dataset))\n",
    "print('Number of validation images: ', len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unexpected type <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_loader))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[48], line 60\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Apply transformation if provided\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 60\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, boxes\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mresize(img, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mantialias)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\transforms\\functional.py:456\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    452\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_size should only be passed if size specifies the length of the smaller edge, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m         )\n\u001b[1;32m--> 456\u001b[0m _, image_height, image_width \u001b[38;5;241m=\u001b[39m get_dimensions(img)\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m    458\u001b[0m     size \u001b[38;5;241m=\u001b[39m [size]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\transforms\\functional.py:80\u001b[0m, in \u001b[0;36mget_dimensions\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mget_dimensions(img)\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mget_dimensions(img)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\transforms\\_functional_pil.py:31\u001b[0m, in \u001b[0;36mget_dimensions\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     29\u001b[0m     width, height \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39msize\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [channels, height, width]\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Unexpected type <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting epoch 1 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/4:   0%|                                                    | 0/65 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 7, 7], expected input[16, 512, 512, 3] to have 3 channels, but got 512 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m lr \u001b[38;5;241m=\u001b[39m get_lr(optimizer)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Forward pass to compute loss.\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m preds \u001b[38;5;241m=\u001b[39m yolo(imgs)\n\u001b[0;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(preds, targets)\n\u001b[0;32m     29\u001b[0m loss_this_iter \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 97\u001b[0m, in \u001b[0;36mYOLOV1.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 97\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m     98\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool1(x)\n\u001b[0;32m    100\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m, in \u001b[0;36mConv.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Aplicamos la convolución y luego la activación ReLU\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[16, 512, 512, 3] to have 3 channels, but got 512 channels instead"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('\\n')\n",
    "    print(f'Starting epoch {epoch + 1} / {num_epochs}')\n",
    "\n",
    "    # Training.\n",
    "    yolo.train()\n",
    "    total_loss = 0.0\n",
    "    total_batch = 0\n",
    "\n",
    "    # Añadir barra de progreso para entrenamiento\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f'Training Epoch {epoch+1}/{num_epochs}', ncols=100)\n",
    "\n",
    "    for i, (imgs, targets) in enumerate(train_loader_tqdm):\n",
    "        # Move to GPU if available\n",
    "        if use_gpu:\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "        # Update learning rate.\n",
    "        update_lr(optimizer, epoch, float(i) / float(len(train_loader) - 1))\n",
    "        lr = get_lr(optimizer)\n",
    "\n",
    "        # Forward pass to compute loss.\n",
    "        preds = yolo(imgs)\n",
    "        loss = criterion(preds, targets)\n",
    "        loss_this_iter = loss.item()\n",
    "        total_loss += loss_this_iter * imgs.size(0)\n",
    "        total_batch += imgs.size(0)\n",
    "\n",
    "        # Backward pass to update model weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the progress bar.\n",
    "        train_loader_tqdm.set_postfix({\n",
    "            'Loss': loss_this_iter, \n",
    "            'Avg Loss': total_loss / float(total_batch), \n",
    "            'LR': lr\n",
    "        })\n",
    "\n",
    "    # Validation.\n",
    "    yolo.eval()\n",
    "    val_loss = 0.0\n",
    "    total_batch = 0\n",
    "\n",
    "    # Añadir barra de progreso para validación\n",
    "    val_loader_tqdm = tqdm(val_loader, desc=f'Validating Epoch {epoch+1}/{num_epochs}', ncols=100)\n",
    "\n",
    "    for i, (imgs, targets) in enumerate(val_loader_tqdm):\n",
    "        # Move to GPU if available\n",
    "        if use_gpu:\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "        # Forward pass to compute validation loss.\n",
    "        with torch.no_grad():\n",
    "            preds = yolo(imgs)\n",
    "            loss = criterion(preds, targets)\n",
    "            loss_this_iter = loss.item()\n",
    "            val_loss += loss_this_iter * imgs.size(0)\n",
    "            total_batch += imgs.size(0)\n",
    "\n",
    "    val_loss /= float(total_batch)\n",
    "\n",
    "    # Print validation loss.\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Val Loss: {val_loss:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Check if GPU devices are available.\n",
    "use_gpu = torch.cuda.is_available()\n",
    "assert use_gpu, 'Current implementation does not support CPU mode. Enable CUDA.'\n",
    "print('CUDA current_device: {}'.format(torch.cuda.current_device()))\n",
    "print('CUDA device_count: {}'.format(torch.cuda.device_count()))\n",
    "\n",
    "# Path to data dir.\n",
    "image_dir = 'data/VOC_allimgs/'\n",
    "\n",
    "# Path to label files.\n",
    "train_label = ('data/voc2007.txt', 'data/voc2012.txt')\n",
    "val_label = 'data/voc2007test.txt'\n",
    "\n",
    "# Path to checkpoint file containing pre-trained DarkNet weight.\n",
    "checkpoint_path = 'weights/darknet/model_best.pth.tar'\n",
    "\n",
    "# Frequency to print/log the results.\n",
    "print_freq = 5\n",
    "tb_log_freq = 5\n",
    "\n",
    "# Training hyper parameters.\n",
    "init_lr = 0.001\n",
    "base_lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 5.0e-4\n",
    "num_epochs = 135\n",
    "batch_size = 64\n",
    "\n",
    "# Learning rate scheduling.\n",
    "def update_lr(optimizer, epoch, burnin_base, burnin_exp=4.0):\n",
    "    if epoch == 0:\n",
    "        lr = init_lr + (base_lr - init_lr) * math.pow(burnin_base, burnin_exp)\n",
    "    elif epoch == 1:\n",
    "        lr = base_lr\n",
    "    elif epoch == 75:\n",
    "        lr = 0.001\n",
    "    elif epoch == 105:\n",
    "        lr = 0.0001\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "# Load YOLO model.\n",
    "yolo = model\n",
    "\n",
    "# Setup loss and optimizer.\n",
    "criterion = Loss(feature_size=yolo.feature_size)\n",
    "optimizer = torch.optim.SGD(yolo.parameters(), lr=init_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "# Load Pascal-VOC dataset.\n",
    "train_dataset = VOCDataset(True, image_dir, train_label)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "val_dataset = VOCDataset(False, image_dir, val_label)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "print('Number of training images: ', len(train_dataset))\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('\\n')\n",
    "    print('Starting epoch {} / {}'.format(epoch, num_epochs))\n",
    "\n",
    "    # Training.\n",
    "    yolo.train()\n",
    "    total_loss = 0.0\n",
    "    total_batch = 0\n",
    "\n",
    "    for i, (imgs, targets) in enumerate(train_loader):\n",
    "        # Update learning rate.\n",
    "        update_lr(optimizer, epoch, float(i) / float(len(train_loader) - 1))\n",
    "        lr = get_lr(optimizer)\n",
    "\n",
    "        # Load data as a batch.\n",
    "        batch_size_this_iter = imgs.size(0)\n",
    "        imgs = Variable(imgs)\n",
    "        targets = Variable(targets)\n",
    "        imgs, targets = imgs.cuda(), targets.cuda()\n",
    "\n",
    "        # Forward to compute loss.\n",
    "        preds = yolo(imgs)\n",
    "        loss = criterion(preds, targets)\n",
    "        loss_this_iter = loss.item()\n",
    "        total_loss += loss_this_iter * batch_size_this_iter\n",
    "        total_batch += batch_size_this_iter\n",
    "\n",
    "        # Backward to update model weight.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print current loss.\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch [%d/%d], Iter [%d/%d], LR: %.6f, Loss: %.4f, Average Loss: %.4f'\n",
    "            % (epoch, num_epochs, i, len(train_loader), lr, loss_this_iter, total_loss / float(total_batch)))\n",
    "\n",
    "        \n",
    "    # Validation.\n",
    "    yolo.eval()\n",
    "    val_loss = 0.0\n",
    "    total_batch = 0\n",
    "\n",
    "    for i, (imgs, targets) in enumerate(val_loader):\n",
    "        # Load data as a batch.\n",
    "        batch_size_this_iter = imgs.size(0)\n",
    "        imgs = Variable(imgs)\n",
    "        targets = Variable(targets)\n",
    "        imgs, targets = imgs.cuda(), targets.cuda()\n",
    "\n",
    "        # Forward to compute validation loss.\n",
    "        with torch.no_grad():\n",
    "            preds = yolo(imgs)\n",
    "        loss = criterion(preds, targets)\n",
    "        loss_this_iter = loss.item()\n",
    "        val_loss += loss_this_iter * batch_size_this_iter\n",
    "        total_batch += batch_size_this_iter\n",
    "    val_loss /= float(total_batch)\n",
    "\n",
    "    \n",
    "    # Print.\n",
    "    print('Epoch [%d/%d], Val Loss: %.4f, Best Val Loss: %.4f'\n",
    "    % (epoch + 1, num_epochs, val_loss))\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
